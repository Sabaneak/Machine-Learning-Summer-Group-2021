{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"5_Logistic_Regression_2.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gEDBT62LQrCz","colab_type":"text"},"source":["# Logistic Regression - 2 (contd.)"]},{"cell_type":"markdown","metadata":{"id":"t76S5rMpQrC0","colab_type":"text"},"source":["## 4. Gradient Descent\n","\n","To minimize our cost, we use Gradient Descent just like before in Linear Regression. \n","\n","> There are other more sophisticated optimization algorithms out there such as Momentum, or RMSProp or ADAM, but you don’t have to worry about these."]},{"cell_type":"markdown","metadata":{"id":"P8jZBDV9QrC1","colab_type":"text"},"source":["### 4.1 Introduction\n","\n","In this era of deep learning, where machines have already surpassed human intelligence it’s fascinating to see how these machines are learning just by looking at examples. When we say that we are training the model, it’s gradient descent behind the scenes who trains it.\n","\n","So let’s dive deeper to have a look at gradient descent.\n","\n","This is what Wikipedia has to say on Gradient descent\n","> Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function\n","\n","This seems little complicated, so let’s break it down.\n","The goal of the gradient descent is to minimise a given function which, in our case, is the loss function of the Logistic Regresion Classifier. To achieve this goal, it performs two steps iteratively.\n","\n","1. Compute the slope (gradient) that is the first-order derivative of the function at the current point\n","2. Move-in the opposite direction of the slope increase (from the current point) by the computed amount\n","\n","<img src=\"https://ik.imagekit.io/xtne6rmcgk/2d_grad_26zdGQaDe.png\">\n","\n","Think of it like this. Suppose you are at top of mountain, and want to get to the bottom of the valley. So you go down the slope. You decide your next position based on your current position and stop when you get to the bottom of the valley which was your goal.\n","\n","Now, consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible.\n","\n","<img src=\"https://ik.imagekit.io/xtne6rmcgk/terrain_jzKTJNL3n.png\">\n","\n","Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.\n","\n","Now the loss of your model depends upon the parameters: weights and biases that we use in computation. It is through gradient descent, we update the parameters of our model to fit it best to the available data. Let's look at that in the next secction"]},{"cell_type":"markdown","metadata":{"id":"AXkkfEe2QrC2","colab_type":"text"},"source":["### 4.2 Mathematics\n","\n","The Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.\n","\n","There are two parameters in our cost function we can control: $w$ (weight) and $b$ (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.\n","\n","We have the following equations:\n","\n","1. $ z = w \\cdot x + b $\n","2. $ \\hat y = S(z) $\n","3. $ \\mathcal{L} (\\hat y, y) = -[y\\log(\\hat y) + (1 - y)\\log(1 - \\hat y)] $\n","\n","and what we need to compute \n","$$ \\frac{\\partial \\mathcal{L}}{\\partial w}, \\frac{\\partial \\mathcal{L}}{\\partial b} $$\n","\n","And for this we will use the chain rule as follows:\n","\n","$$ \\implies \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{d\\mathcal{L}}{d\\hat y} \\cdot \\frac{d \\hat y}{dz} \\cdot \\frac{\\partial z}{\\partial w} $$\n","\n","and\n","\n","$$ \\implies \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{d\\mathcal{L}}{d\\hat y} \\cdot \\frac{d \\hat y}{dz} \\cdot \\frac{\\partial z}{\\partial w} $$\n","\n","> Solve these on your own and get to the requires solution. It might be tricky, but the concept from  MATH F111 and MATH F112 should help\n","\n","> Hint 1: The derivative o the sigmoid function is given by:\n","\n","> $$ \\frac{dS(z)}{dz} = S(z)(1 - S(z)) $$\n","\n","\n","> Hint 2: \n","For reasons you will soon see, the shape of $ \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $ w $ must be equal. Similarly for $b$\n","\n","The Final solution for these equations has the following structure:\n","\n","$$ \\implies \\frac{\\partial \\mathcal{L}}{\\partial w} = (\\hat y - y) \\cdot x^T $$\n","\n","$$ \\implies \\frac{\\partial \\mathcal{L}}{\\partial b} = (\\hat y - y) $$\n","\n","> Note: In all the code that you will write, $d\\theta$ will refer to the derivative of the cost wrt $\\theta$"]},{"cell_type":"code","metadata":{"id":"9hN0InzlQrC3","colab_type":"code","colab":{}},"source":["def compute_derivative_sgd(x, a, y):\n","    \"\"\"\n","    Compute dw and db\n","    \n","    Input:\n","        x: Input Vector\n","        y: Known class, 0 or 1\n","        a: Calculated probability of belonging to class 1\n","    Output:\n","        dw, db: grads, Derivatives of cost wrt w and b  \n","    \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9_5q_xFYQrC9","colab_type":"text"},"source":["Now, we must update our parameters $ w $ and $ b $ by \"Taking a step down\" the slope. We do this by subtracting the derivatives with the respective Paramters:\n","\n","$$ w := w - \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n","$$ b := b - \\frac{\\partial \\mathcal{L}}{\\partial b} $$"]},{"cell_type":"markdown","metadata":{"id":"TqKxpLUeQrC-","colab_type":"text"},"source":["### 4.3 Learning Rate\n","\n","Coming back to the mountain trekking analogy. Now imagine, instead of you there is a giant with huge fucken feet, and an ant trying to reach the same ending point, the valley (minima) from the same starting point, top of the mountain (maxima). Now, notice that the giant can never reach the valley and will always keep moving somewhere around him, no matter how small of a step he takes owing to the sheer size of his feet. And on the other hand, the ant being that small will take a very long time to reach the valley, but will reach never the less. So the problem in this case is the scale of the steps that the person can take.\n","\n","Similary, depending upon the context, the slope or the gradient might be too large or too small to reach the minima. So, to deal with the problem we scale the slope with a constant amount, before updating the parameters of the model. This scale factor is called the learning rate, denoted by $\\alpha$.\n","\n","So, the resulting expression is\n","\n","$$ \\theta := \\theta - \\alpha \\frac {\\partial \\mathcal {L}}{\\partial \\theta} $$ where $\\theta$ denotes the parameters of the model, $w$ and $b$.\n","\n","> Note: It is almost always that the slope is too large to reach the gradient, and you will almost never encounter a learning rate greater than 1\n","\n","Abstracting the analogy to provide intuition:\n","\n","1. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. \n","2. With a low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. \n","3. A very low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\n","\n","> A low learning rate might also get stuck in a local minima, as it is also a point of 0 gradient.\n","This is why we used a log loss. It has only a gloabal minimum, and not local Minima where we can get stuck"]},{"cell_type":"markdown","metadata":{"id":"5VSrbVAYQrC_","colab_type":"text"},"source":["So, the final algorithm for the gradient descent Optimizelooks like: \n","\n","$$ w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n","\n","\n","$$ b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b} $$\n","\n","This process must be repeat thousands of time, before we can reach the minima."]},{"cell_type":"code","metadata":{"id":"dS9RqLuYQrDA","colab_type":"code","colab":{}},"source":["def update_params_sgd(w, b, dw, db, learning_rate):\n","    \"\"\"\n","    Update the parameters\n","    Inputs:\n","        w: weight matrix\n","        b: bias\n","        dw: grad of w\n","        db: derivative of b\n","        learning_rate:\n","    \"\"\"\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsvyecY3QrDE","colab_type":"text"},"source":["## 5. Training your Model\n","\n","Now that you are reasonably familiar with gradient descent, you will see how we will optimize our model. \n","\n","Lets first look at the the word dataset, in the context of machine learning. \n","\n","Consider you want to classify a given set of drawings as \"Image of cat\", or \"Not Image of cat\". This is one of the many usecases of Logistic Regression. Consider that each drawing is done in a canvas of size 28x28 pixels, and is a graysale image.\n","\n","CAT | NOT CAT\n","- | -\n","![cat](https://ik.imagekit.io/xtne6rmcgk/cat_HsJnkKIku.png) | ![not cat](https://ik.imagekit.io/xtne6rmcgk/book_75Z0TQKQn.png)\n","\n","Now a logistic regression algorithm, can not handle 2D data, so you flatten the image and use that as your feature vector. So, in this case, $n_x = 28 \\cdot 28 = 784 $ and each sample of your dataset has the shape $(784, 1)$. Also, you typically have thousands, or even hundred thousands of data samples, to optimize your model.\n"]},{"cell_type":"markdown","metadata":{"id":"xk4iA5EgQrDF","colab_type":"text"},"source":["### 5.1 Optimizing your model\n","\n","Optimizing the Model is achieved via reducing the loss to minimum via an optimization algorithm. The optimization algorithm we will be using is gradient descent. \n","\n","So, now you need to make a Logistic Regression model. This model will have a weights matrix, $w$ of shape $(1, 784)$ and a bias, $b$. In the beginning, these parameters will have small random values and over the several iterations of gradient descent, these parameters will be optimized for classification. \n","\n","The final algorithm for the Gradient Descent in pseudo code looks like this:\n","\n","```python\n","X_train, Y_train = get_training_dataset() # X has shape (n_x, m) and Y as shape (1, m)\n","w, b = initialize_parameters()\n","alpha = 0.5 # Or any small value, can be tuned for better performance\n","\n","for i in (0, X.shape[1]):\n","    x = X_train[:, i]\n","    y = Y_train[:, i]\n","    a = forward_propagate(w, b)\n","    dw, db = backward_propagate(a, y, x)\n","\n","    w = w - alpha * dw\n","    b = b - alpha * db\n","\n","```\n","> Try to get accustomed to the code, and try figuring out what the forward_propagate and backward_propagate functions should look like. \n","\n","### This method, of iterating over 1 example at a time is called stochastic gradient descent."]},{"cell_type":"markdown","metadata":{"id":"3115SN0WQrDF","colab_type":"text"},"source":["### 5.2 Splitting the dataset - Train and Test sets\n","\n","Now, notice that while procuring the data, I called them $X_{train}$, and $Y_{train}$. This is beacuse this data comprises what is called the \"Training dataset\" or the \"Train set\".\n","\n","In machine learning, any dataset you have is broadly split into two different parts, training data and testing data. Training data is used to optimize your model and enable it to reach the global minima.  The testing data is used to determine the accuracy of your model. The testing data, is the subset of your dataset, which your model, has never \"seen\" before. That is, the Logistic regression model was never trained on the testinfg data.\n","\n","The training and testing data are not evenly split. Your main dataset is typically split in the 8:2 ratio, or 9:1 ratio. For example, if you have 10,000 data points, your training set has size 8000, and test set has size 2000. \n","\n","The number of examples you have in your train set, or the number of samples in your train set is denoted by $m$.\n","\n","And, the number of samples in your entire dataset be denoted by $M$.\n","#### VERY IMPORTANT NOTE: NEVER TRAIN YOUR MODEL ON YOUR TEST SET \n","\n","> For higher dimensional and more complicated models, the data is split in 3 parts into train, dev, and test set. The purpose of the dev set is beyond the scope of this course.\n","> In case you have a much bigger dataset, say of the size 5 million data points, the split is much more skewed, for exmaple 99:1, or 99 : 0.5 : 0.5.\n"]},{"cell_type":"markdown","metadata":{"id":"YvdZxt7PQrDG","colab_type":"text"},"source":["In summary, what we have done till now is using forward propagation calculated the Linear Function, $z$ and the output $\\hat y$, or $a$, for one data sample at a time. Then calculated the derivatives using backpropagation, and updated the parameters. \n","\n","\n","Now, while using the algorithm you will find that the training process is quite slow. This is because iterating to each training example one by one, is very time consuming. To replace this, we will use a new algorithm to train our data. This approach is a vectorized implementation of the Stochastic Gradient Descent Process, and takes advantage of the fast matrix multiplication implementations of numpy. "]},{"cell_type":"markdown","metadata":{"id":"EpXNp173QrDH","colab_type":"text"},"source":["## 6. Batch Gradient Descent\n","\n","Till now the shapes of the different elemets of our gradient decent algorithm have been:\n","1. $x$.shape = $(n_x, 1)$\n","2. $w$.shape = $(1, n_x)$\n","3. $b$ is a 1D scalar\n","4. $y$ is a 1D scalar\n","5. $\\hat y$ or $a$ is a 1D scalar\n","\n","\n","What we will do in this case, is instead of iterating over one data sample at a time, we process the entire training dataset at once. \n","\n","Let's denote the entire input data (ie all the different samples of input data $x$, of shape $(n_x, 1)$, stacked in columns against one another) by $X$. So, $X$ has shape $(n_x, M)$. And Splitting the data into train and test batches gives the following results, \n","\n","1. $X_{train}$ has shape = $(n_x, m)$\n","2. $X_{test}$ has shape = $(n_x, M - m)$\n","\n","And similarly stacking all the $y$s, gives $Y$ of shape $(1, M)$. And splitting those,\n","\n","1. $Y_{train}$ has shape = $(1, m)$\n","2. $Y_{test}$ has shape = $(1, M - m)$\n","\n","Each individual example is denoted by a **superscipted parantheses** \n","ex: $X_{train}^{(i)}$ represents the $i^{th}$ training sample, and $ i\\in (0, m]$ \n","\n","1. $X^{(i)}$ has the shape $(n_x, 1)$\n","2. $Y^{(i)}$ has the shape $(1, 1)$ and is basically a scalar\n","\n","This vectorization also changes the mathematics slightly. The cost function and the gradient changes slightly. Lets Look at that next."]},{"cell_type":"markdown","metadata":{"id":"Yqe7j86iQrDH","colab_type":"text"},"source":["### 6.1 Cost Function for Batch Gradient Descent\n","\n","Till now we were dealing with single examples and the loss automatically turned out to be a scalar value. The cost of the model, although used synonymously to loss till now, is a contextually different variable. \n","\n","The difference between them is subtle. Loss is the special \"distance\" function calculated between a single output sample, and its predicted value. Cost on the other hand, is the mean of the loss over your entire training set. For stochastic gradient descent, since the training sample consisted of the single data point, it's mean was itself, and hence the loss was the cost. However, now when we are optimizing the model based on the m samples from the dataset at once, we will take the mean of the loss over all the samples.\n","\n","Hence, \n","The loss is given by: \n","\n","$$ \\mathcal{L} (A^{(i)}, Y^{(i)}) = -[Y^{(i)}\\log(A^{(i)}) + (1 - Y^{(i)})\\log(1 - A^{(i)})] $$\n","\n","and the cost is given by:\n","\n","$$ J = \\frac{-1}{m} \\sum_1^m {\\mathcal{L} (A^{(i)}, Y^{(i)})} $$\n","\n","$$ \\implies J = \\frac{-1}{m} \\sum_1^m {Y^{(i)}\\log(A^{(i)}) + (1 - Y^{(i)})\\log(1 - A^{(i)})} $$\n","\n","> While training, it is the cost that we aim to reduce to minimum."]},{"cell_type":"markdown","metadata":{"id":"it81S5qCQrDI","colab_type":"text"},"source":["### 6.2 Gradient Calculation\n","\n","From the previous calculations, Forward Propagation has the following equations:\n","\n","$$Z = w \\cdot X + b$$\n","$$\\hat Y = A = S(Z)$$\n","$$ J = \\frac{-1}{m} \\sum_1^m {Y^{(i)}\\log(A^{(i)}) + (1 - Y^{(i)})\\log(1 - A^{(i)})} $$\n","\n","> Note: You might notice that although, $w \\cdot X$ is a $(1, m)$ row vector, b is a scalar. So how does the addition operation take place? In reality b must also be a row vector of the same shape, ie $(1, m)$. To achieve this you can multiply b to a row vector of ones with the shape $(1, m)$. However, in numpy such operation are already predefined and you need not worry about it. The scalar is \"broadcasted\" into the required shape and the operation perfromed automatically.\n","\n","And the derivatives wee need to compute are $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$.\n","\n","The calculation of these derivatives although, not as straight forward as earlier is a good exercise and left to the reader. \n","\n","These derivatives, are calculated as follows:\n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} (A - Y) \\cdot X^T $$\n","\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_1^m (A - Y) $$\n","\n","Which, are incredibly similar to the results seen earlier. \n","\n","And, finally the Batch gradient descent algorithm results in the following,\n","\n","$$ w := w - \\alpha \\frac{\\partial J}{\\partial w} $$\n","\n","$$ b := b - \\alpha \\frac{\\partial J}{\\partial b} $$\n","\n","With this, we come to the end of the gradient Descent Algorithm."]},{"cell_type":"markdown","metadata":{"id":"78a56dGWQrDJ","colab_type":"text"},"source":["## 7. Testing your model and Predicting Values\n","\n","Predicitions from your Logistic Regression Model are very easy to compute, and all you need to do is compute the probability and check if it is above the predefined threshold value, usually 0.5. \n","\n","Quantiatively, \n","\n","$$\\hat Y = S(w \\cdot X + b)$$\n","\n","$$\n","\\begin{equation}\n","    Y_{prediction}^{(i)} =\n","    \\begin{cases} \n","        1 \\text{, if } \\hat Y^{(i)} \\ge 0.5\\\\\n","        0 \\text{, if } \\hat Y^{(i)} \\lt 0.5\\\\\n","    \\end{cases}\n","\\end{equation}\n","$$\n"," \n"," \n","For testing your model, compute the $ Y_{prediction, testing}^{(i)}$ on your testing data. From this, element wise comparison can be drawn and accuracy of your model evaluated. "]},{"cell_type":"markdown","metadata":{"id":"SjirMyX-QrDJ","colab_type":"text"},"source":["## Notation Convention Revision\n","\n","Notation | Meaning | Shape or type\n","---|---|---\n","$ x $| Single Input, Feature Vector | $ (n_x, 1) $\n","$ X $| Multiple Input Vectors, Matrix of Feature Vectors | $ (n_x, M) $\n","$ X^{(i)} $| $i^{th}$ Feature Vector of X | $ (n_x, 1) $\n","$ y $| Label or Class of Input Vector x, 0 or 1 | Scalar\n","$ Y $| Lables of $X$| $ (1, M) $ \n","$ Y^{(i)} $| Lables of $X^{(i)}$| scalar \n","$ a $| Probability of $x$ belonging to class 1 or having label 1 | Scalar\n","$ A $| Array of Probabilies of $X^{(i)}$ having label 1| $ (1, M) $ \n","$ A^{(i)} $| Probability of $X^{(i)}$ having label 1 | scalar \n","$ w $| Weight Matrix | $ (1, n_x) $ \n","$ b $| Bias | scalar\n","$\\frac {\\partial J}{\\partial w}$ | Gradeint of Cost function wrt Weigth Matrix | $(1, n_x)$ \n","$\\frac {\\partial J}{\\partial b}$ | Gradeint of Cost function wrt Bias | scalar\n"]}]}