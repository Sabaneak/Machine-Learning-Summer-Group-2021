{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Logistic Regression is a method when the variable to be determined(the dependent variable) is categorical. It might be helpful to think of it as a probabilistic classification model. It is used to assign observations to a discrete set of classes. Logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes. For example, To predict\n",
    "\n",
    "1. If an email is spam(1) or not(0)\n",
    "2. If a given image is of a cat(1) or not(0)\n",
    "3. If a tumor is malignant(1) or not(0)\n",
    "\n",
    "> Although multiclass classification is possible as well (ex: Movie Rating from 1 to 5, or food preference between vegan, veg or non-veg) it is out of scope for now. We will be taking it up in the subsequent notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Comparison To Linear Regression\n",
    "\n",
    "Where Linear Regression is used to predict continuous numerical values, Logistic Regression's predictions are discrete (only certain values or categories are allowed). It represents the probability of the sample belonging to a particular category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Binary Logistic Regression\n",
    "Say you are given data on student exam results, and your goal is to predict whether a student will pass or fail based on the number of hours slept, and the hours studied. In this case we have,\n",
    "1. Features: \n",
    " - Hours Slept\n",
    " - Hours Studied\n",
    "2. Classes:\n",
    " - Pass(1)\n",
    " - Fail(0)\n",
    " \n",
    "Ex:\n",
    "\n",
    "Studied (hrs) | Slept (hrs) | Result |\n",
    "--------------| ----------- | -------|\n",
    "4.85 | 9.63 | 1\n",
    "8.62 | 3.23 | 0\n",
    "5.43 | 8.23 | 1\n",
    "st | sl | ?\n",
    "\n",
    " \n",
    "So in this case we need to calculate that given a student has slept for $ sl $ hours and studied for $ st $ for a test what is the probability that he will pass, and what is the probability that he will fail. More Formally,\n",
    "\n",
    "$$ P(result = Pass | Slept = sl, Studied = st) $$ and\n",
    "$$ P(result = Fail | Slept = sl, Studied = st) $$\n",
    "\n",
    "Or, abstracting into variables, \n",
    "\n",
    "$$ p = P(Y = 1 | x_1, x_2, ..., x_n) $$ \n",
    "$$ q = P(Y = 0 | x_1, x_2, ..., x_n) $$\n",
    "\n",
    "where, \n",
    "> Y is the class that the sample data belongs to\n",
    "> $x_i$ are the various features of the data\n",
    "\n",
    "Now, notice that since $p$ and $q$ are the only possible classes and represent probabilities,\n",
    "\n",
    "$$ p + q = 1 $$ \n",
    "\n",
    "and calculating any one of them is sufficient to find the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation\n",
    "\n",
    "Forward propagation is the alogrithm that we will use to calculate the probabilites that we have discussed till now. It involves two step:\n",
    "\n",
    "1. Calculating a Linear Function\n",
    "2. Applying the activation function to it. \n",
    "\n",
    "Lets, look at them in the next section.\n",
    "\n",
    "> Forward Propagation (and backward propagation) is a concept that you will encounter mostly in Neural Networks (in the next Topic) an this usually not associated with Logistic Regression. But, to make the transition easier to the next topic, we will refer to the steps as they would in a Neural Network Context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Linear Function \n",
    "\n",
    "The first Step in creating a Logistic regression Model is similar to a linear regression Model. We need to take a linear combination of all the features, and a constant amount to it. In Linear Regression you saw them called as parameters, labeled as $\\theta$:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n $$\n",
    "or, in vectorized form:\n",
    "$$ h_\\theta(x) = x \\cdot \\theta $$\n",
    "\n",
    "From here on we will be using the notation more commonly used in Neural Network Architectures. \n",
    "The $\\theta_0$ term is called the bias, and the parameters $\\theta_1, \\theta_2, ..., \\theta_n $ the weights.\n",
    "\n",
    "The Output of the linear Function is now represented as: \n",
    "\n",
    "$$ z(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    "> where:\n",
    "1. $z$ is a scalar,\n",
    "2. $w_i$ are the scalar weights\n",
    "3. $b$ is the sclar bias\n",
    "\n",
    "Now, this can also be written as a matrix product as follows:\n",
    "\n",
    "$$ z = w \\cdot x + b $$\n",
    "\n",
    "> where,\n",
    "1. $x$ = feature vector of shape $(n_x, 1) = (x_1, x_2, ..., x_n)$ stacked in a column\n",
    "2. $w$ = weight matrix of shape $ (1, n_x) = [[w_1], [w_2], ..., [w_3]]$ stacked in a row\n",
    "3. $b$ = scalar bias\n",
    "\n",
    "And as always, a linear function is unbounded and so, \n",
    "$$ z \\in (-\\infty, \\infty) $$\n",
    "\n",
    "> If at any point you are confused with notation, check the Notation Section at the bottom. There is a quick reference there, and if the doubt still persists PM one of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation Function: The Sigmoid Function\n",
    "As you already saw in Linear Regression, the function $z = w \\cdot x + b$ is unbounded. However, since we are calculating the probability, we need to scale the value of $Z$ (obtained from Linear Regression) to the range $[0, 1]$. To achieve this we use the Sigmoid function.\n",
    "\n",
    "The sigmoid, or the logistic Function is a special function which maps its input to values between 0 and 1. It is an S shaped curve defined by the following equation:\n",
    " \n",
    "$$ S(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    " \n",
    "> Notes:\n",
    "> 1. $S(Z) \\in [0, 1] $\n",
    "> 2. z = Input to your function (The prediction of the linear regression model, ie: $wx + b$\n",
    "\n",
    "<img src=\"https://ik.imagekit.io/xtne6rmcgk/sigmoid_L64WrfZy6d.png\">\n",
    "\n",
    "> Try to find other functions with similar properties, or similar shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calculates element wise Sigmoid for the entire numpy array\n",
    "    \n",
    "    Input:\n",
    "        z: Numpy array of unknown dimensions\n",
    "    Output:\n",
    "        s: Numpy array of element wise sigmoid of z \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Decision Boundary\n",
    "Our current Prediction returns a probability between 0 and 1. In order to map this value to a discrete class, we select a threshold value or the tipping point. All values above this tipping point are classified into \"Class 1\", and all below to \"Class 2\".\n",
    "\n",
    "For example if our threshold value was 0.5,\n",
    "\n",
    "$$ p \\ge 0.5 \\implies class = 1 $$\n",
    "$$ p \\lt 0.5 \\implies class = 2 $$\n",
    "\n",
    "<img src=\"https://ik.imagekit.io/xtne6rmcgk/dec_bound_x1h0DJ13-6.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Making Predictions\n",
    "\n",
    "Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function. A prediction function in logistic regression returns the probability of our observation being positive, True, or “Yes”. We call this class 1 and its notation is $P(class=1)$.\n",
    "\n",
    "As the probability gets closer to 1, our model is more confident that the observation is in class 1.\n",
    "\n",
    "Given: the feature vector, weight matrix and bias\n",
    "1. Linear function: \n",
    "    $$ z = w\\cdot x + b $$\n",
    "2. Applying activation: \n",
    "    $$ P(class = 1) = a = \\hat y = S(z) $$\n",
    "3. Check with threshold Value:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_{prediction} =\n",
    "    \\begin{cases} \n",
    "        1 \\text{, if } a \\ge 0.5\\\\\n",
    "        0 \\text{, if } a \\lt 0.5\\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w, b, x):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities and Predictions corresponding to the x\n",
    "    \n",
    "    Input:\n",
    "        w: Weights Matrix\n",
    "        b: bias\n",
    "        x: Input Vector\n",
    "    Returns:\n",
    "        a: Probability of x belonging to class 1\n",
    "        y_prediction: Class that x belongs\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 The Loss Function: Log Loss\n",
    "\n",
    "For a linear regression model you used the MSE Loss. However, the same can not be applied here. Why? There is a great math explanation in chapter 3 of Michael Neilson’s deep learning [book](http://neuralnetworksanddeeplearning.com/chap3.html), but for now I’ll simply say it’s because our prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.\n",
    "\n",
    "Instead of the MSE we will use a loss function called the Cross-Entropy Loss or the Log Loss, denoted by $\\mathcal{L}(\\hat y, y)$. Cross Entropy loss can be divided into two separate loss functions: one for y = 1, and other for y = 0.\n",
    "\n",
    "1. if $y = 1$:\n",
    "$$ \\mathcal{L}(\\hat y, 1) = -\\log(\\hat y) $$ \n",
    "2. if $y = 0$:\n",
    "$$ \\mathcal{L}(\\hat y, 0) = -\\log(1 - \\hat y) $$ \n",
    "\n",
    "The benefits of taking the logarithm reveal themselves when you look at the loss function graphs for $y=1$ and $y=0$. These smooth monotonic functions (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost. \n",
    "\n",
    "-|-\n",
    "- | - \n",
    "![alt](https://ik.imagekit.io/xtne6rmcgk/log_y__TYrTvZTL-.png) | ![alt](https://ik.imagekit.io/xtne6rmcgk/log2_uvK-wwZ7Y.png)\n",
    "\n",
    "These expressions can be combined to give:\n",
    "\n",
    "$$ \\mathcal{L} (\\hat y, y) = -[y\\log(\\hat y) + (1 - y)\\log(1 - \\hat y)] $$\n",
    "\n",
    "> Exercise: Prove the above is true\n",
    "\n",
    "> Loss function and Cost function are almost synonymous, with a small caveat that we will get to later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a, y):\n",
    "    \"\"\"\n",
    "    Calculates Cross entropy Loss\n",
    "    \n",
    "    Input:\n",
    "        y: Known class, 0 or 1\n",
    "        a: Calculated probability of belonging to class 1\n",
    "    Returns:\n",
    "        loss: Cross Entropy Loss\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
