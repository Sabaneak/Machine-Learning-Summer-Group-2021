{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Copy of 5_Logistic_Regression_Assignment.ipynb","provenance":[{"file_id":"1c74yxatTWIIlcUkXbUi6YWhKMjpBz322","timestamp":1590565453013}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QEncmBStFCLo","colab_type":"text"},"source":["# Logistic Regression Assignment\n","\n","## General Instructions for this assignment:\n","1. Don't change the function signature\n","2. Don't change the code already given to you; A lot of the work has been done for you, as we assume you are reasonably familiar with it.\n","3. Write the code only where asked"]},{"cell_type":"markdown","metadata":{"id":"ODV-kLWnFCLp","colab_type":"text"},"source":["## 1. Basic Design of the Model\n","1. Output of the model: 0 or 1 (Binary Classification)\n","2. Hypothesis to be tested: $Z = W \\cdot X + b$\n","3. Activation Function: $\\frac{1}{1 + e^{-x}} $ (Signmoid Function)"]},{"cell_type":"markdown","metadata":{"id":"_oehNhBDFCLq","colab_type":"text"},"source":["## 2. Import Packages\n","\n","1. numpy: obviously\n","2. matplotlib: for making graphs\n","3. seaborn: Making matplotlib graphs prettier"]},{"cell_type":"code","metadata":{"id":"OtEoCnoZFCLr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","# Next Libraries are unimportant, they just make everyhting look better\n","\n","import matplotlib.style as style\n","import seaborn as sns\n","\n","style.use('seaborn-poster') #sets the size of the charts\n","style.use('ggplot')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BF5Hf3UhFCLv","colab_type":"text"},"source":["## 3. Loading the dataset\n","\n","We have prepared and preprocessed the data. You can use the `np.load(path)` to obtain a dictionary of numpy arrays.\n","\n","> The loading has been done for you, just ensure that the path is correct and obtain the nump arrays from the dictionary"]},{"cell_type":"code","metadata":{"id":"5rPC9-xZFCLx","colab_type":"code","colab":{}},"source":["dataset = np.load('quickdraw/10k/dataset.npz')\n","\n","## Get the numpy arrays from the dictionary\n","X_train = None\n","Y_train = None\n","X_test  = None\n","Y_test  = None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOS8h4viFCL0","colab_type":"code","colab":{}},"source":["print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Bjoq3CUFCL4","colab_type":"text"},"source":["If you loaded the data correctly, then the code below should display\n","```\n","(784, 8000)\n","(1, 8000)\n","(784, 2000)\n","(1, 2000)\n","```"]},{"cell_type":"markdown","metadata":{"id":"QwRecXxsFCL4","colab_type":"text"},"source":["## 4. Get a feel of the data\n","\n","### 5.1 View the Imagethat\n","Use the imshow function of matplotlib.pyplot to actually see one of the 28x28 images from the training dataset and its respectie label."]},{"cell_type":"code","metadata":{"id":"ozMfJJW3FCL5","colab_type":"code","colab":{}},"source":["idx = np.random.randint(X_train.shape[1])\n","\n","plt.imshow(X_train[:, idx].reshape(28, 28))\n","\n","label = \"cat\" if Y_train[:, idx][0] else \"bat\"\n","print(f\"Label: {label}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCQ6-6NsFCL9","colab_type":"text"},"source":["### View the raw data\n","\n","View the actual raw array associated with the idx selected previously"]},{"cell_type":"code","metadata":{"id":"mYlHFjL3FCL9","colab_type":"code","colab":{}},"source":["X_train[:, idx]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeg9vFx5FCMA","colab_type":"text"},"source":["## 6. Normalizing the data\n","\n","Normalizing the data with the following equation:\n","\n","$$ X_{norm} = \\frac {X - X_{min}}{X_{max} - X_{min}} $$\n","\n","For this pixel data, $X_{max} = 255$ and $X_{min} = 0$\n","\n","> After running the next cell, go back and view the raw array again"]},{"cell_type":"code","metadata":{"id":"j9LgEtNsFCMB","colab_type":"code","colab":{}},"source":["## Normalize the training and testning data\n","\n","# Replace None with relevant code\n","X_train = None\n","X_test = None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FB-J1xHEFCMF","colab_type":"text"},"source":["## 7. Helper functions for the Model:\n","\n","### Write the respective functions described by their docstrings"]},{"cell_type":"code","metadata":{"id":"Y1T3thCFFCMG","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    \"\"\"\n","    Computes the element sigmoid of scalar or numpy array(element wise)\n","    \n","    Arguments:\n","        z: Scalar or numpy array\n","    \n","    Returns:\n","        s: Sigmoid of z (element wise in case of Numpy Array)\n","    \"\"\"\n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BrrJqSIuFCMJ","colab_type":"code","colab":{}},"source":["def initialize_parameters(n_x):\n","    \"\"\"\n","    Initialize w to a zero vector, and b to a 0 with datatype float \n","    \n","    Arguments:\n","        n_x: Number of features in each sample of X\n","    \n","    Returns:\n","        w: Initialized Numpy array of shape (1, n_x) (Weight)\n","        b: Initialized Scalar (bias)\n","    \"\"\"\n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    return w, b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Th4xkEvrFCMM","colab_type":"code","colab":{}},"source":["def compute_cost(A, Y):\n","    \"\"\"\n","    Calculate the Cost using the Cross Entropy Loss\n","    \n","    Arguments:\n","        A: Computer Probabilities, numpy array\n","        Y: Known Labels, numpy array\n","        \n","    Returns:\n","        cost: The computed Cost\n","    \"\"\"\n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    return np.squeeze(J)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B51HHAg_FCMQ","colab_type":"text"},"source":["Here is a summary of the equations for Forward Propagation and Backward Propagation we have used so far:\n","\n","For m training examples $ X_{train} $ and $ Y_{train} $:\n","\n","### 7.1 Forward Propagation\n","\n","$$ Z^{(i)} = w \\cdot X_{train}^{(i)} + b $$\n","\n","$$ \\hat Y^{(i)} = A^{(i)} = \\sigma(Z^{(i)}) = sigmoid(Z^{(i)}) $$\n","\n","$$ \\mathcal{L}(\\hat Y^{(i)}, Y_{train}^{(i)}) = \\mathcal{L}(A^{(i)}, Y_{train}^{(i)}) = -[Y_{train}^{(i)} \\log(A^{(i)}) + (1 - Y_{train}^{(i)}) \\log(1 - A^{(i)})] $$\n","\n","$$ J = \\frac{1}{m} \\sum_1^m \\mathcal{L} (A^{(i)}, Y_{train}^{(i)}) $$\n","\n","\n","### 7.2 Backward Propagation - Batch Gradient Descent\n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} (A - Y) \\cdot X^T $$\n","\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_1^m (A - Y) $$\n","\n","\n","> Note: Represent $ \\frac{\\partial J}{\\partial w} $ as dw, and $ \\frac{\\partial J}{\\partial b}$ as db\n"]},{"cell_type":"code","metadata":{"id":"xKf1Dc2lFCMQ","colab_type":"code","colab":{}},"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Perform forward and backward propagation for the Logistic Regression model\n","    \n","    Arguments:\n","        w: The Weight Matrix of dimension (1, n_x)\n","        b: Bias\n","        X: Input Matrix, with shape (n_x, m)\n","        Y: Label Matrix of shape (1, m)\n","        \n","    Returns:\n","        dw: Gradient of the weight matrix\n","        db: Gradient of the bias\n","        cost: Cost computed on Calculated Probability, and output Label\n","    \"\"\"\n","    \n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    assert dw.shape == w.shape\n","    return dw, db, cost\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJwQjwOiFCMU","colab_type":"text"},"source":["### 7.3 Optimization\n","\n","For a parameter $ \\theta $, the gradient descent update rule is given by:\n","$$ \\theta := \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta} $$\n","\n","where $\\alpha$ is the learning rate"]},{"cell_type":"code","metadata":{"id":"FYFW6bSkFCMV","colab_type":"code","colab":{}},"source":["def fit(w, b, X, Y, num_iterations, learning_rate, print_freq=100):\n","    \"\"\"\n","    Given the parameters of the model, fit the model corresponding to the given Input Matrix aand output labels, by performing batch gradient descent for given number of iterations.\n","    \n","    Arguments:\n","        w: The Weight Matrix of dimension (1, n_x)\n","        b: Bias\n","        X: Input Matrix, with shape (n_x, m)\n","        Y: Label Matrix of shape (1, m)\n","        num_iterations: The number of iteratios of bgd to be performed\n","        print_freq: Frequency of recording the cost\n","    Returns:\n","        w: Optimized weight matrix\n","        b: optimized bias\n","        costs: print the cost at frequency given by print_freq, no prints if freq is 0\n","    \"\"\"\n","    \n","    costs = []\n","    for i in range(num_iterations):\n","        ## 1. Calculate Gradients and cost\n","        ### Write Code here ###\n","    \n","        ### End Code here ###\n","\n","        costs.append(cost)\n","            \n","        if print_freq and i % print_freq == 0:\n","            print(f\"Cost after iteration {i}: {cost}\")\n","                \n","        ## 2. Update parameters\n","        ### Write Code here ###\n","    \n","        ### End Code here ###      \n","\n","            \n","    return w, b, costs\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oBa_RviiFCMY","colab_type":"text"},"source":["### 7.4 Prediction\n","Use the following equation to determine the class that a given sample belongs to:\n","\n","$$\n","\\begin{equation}\n","    Y_{prediction}^{(i)} =\n","    \\begin{cases} \n","        1 \\text{, if } \\hat Y^{(i)} \\ge 0.5\\\\\n","        0 \\text{, if } \\hat Y^{(i)} \\lt 0.5\\\\\n","    \\end{cases}\n","\\end{equation}\n","$$\n","\n","> Hint: Use boolean Masking"]},{"cell_type":"code","metadata":{"id":"fcLm6a7cFCMY","colab_type":"code","colab":{}},"source":["def predict(w, b, X):\n","    \"\"\"\n","    Predict the class which the given feature vector belongs to given Weights and Bias of the model\n","    \n","    Arguments:\n","        w: The Weight Matrix of dimension (1, n_x)\n","        b: Bias\n","        X: Input Matrix, with X.shape[0] = n_X\n","    Returns:\n","        Y_prediction: Predicted labels\n","    \"\"\"\n","    \n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    return Y_prediction"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dv7psRMoFCMb","colab_type":"text"},"source":["## 8. Building the Model\n","\n","Now we have assembled all the individual pieces required to create the Logistic Regression model.\n","Next function is creating the model and calculating its train and test accuracy. \n","\n","Acuracy is the ratio of correctly guessed samples, to the total number of samples that were checked. \n","> Note: The correct accuracy is not being provided this time."]},{"cell_type":"code","metadata":{"id":"5RUTGfTyFCMc","colab_type":"code","colab":{}},"source":["def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_freq):\n","    \"\"\"\n","    Create a model and fit it to the train and test data. Use this model to compute the train and test accuracy after 2500 iterations\n","    \n","    Arguments:\n","        X_train: Training Data X\n","        Y_train: Training Data Y\n","        X_test: Testing Data X\n","        Y_test: Testing data Y\n","        num_iterations: Number of iterations of bgd to perform\n","        learning_rate: Learning Rate of the model\n","        print_freq: Frequency of recording the cost\n","    Returns:\n","        -None-\n","    \"\"\"\n","    \n","    ### Write Code here ###\n","    \n","    ### End Code here ###\n","    \n","    Y_prediction_train = predict(w, b, X_train)\n","    Y_prediction_test = predict(w, b, X_test)\n","    \n","    costs = np.squeeze(costs)\n","    \n","    print(f\"train accuracy: {100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100} %\")\n","    print(f\"test accuracy: {100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100} %\")\n","    \n","    plt.plot(costs)\n","    \n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (per hundreds)')\n","    plt.title(f\"Learning rate = {learning_rate}\")\n","    plt.show()\n","       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEmGRqtqFCMf","colab_type":"code","colab":{}},"source":["model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.1, print_freq=100)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4U150gDFCMi","colab_type":"text"},"source":["## 9. Experiments (Not Graded)\n","\n","Try using different Learning Rates, Change the number of iterations, try a different initialization of the parameters, try Stochastic Gradient Descent on the same Model. The possibilities are endless, and we encourage you to explore as much as you can. You can even try to use the Logistic Regression Model implementation from the ScikitLearn library, or explore other optimizing algorithms! If you try something new, do send it along with the assignment. We'd love to see what kinda work you are doing! Till then, adios!"]}]}